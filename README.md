# The Last Mile: Evaluating Open Source Computer Vision Tools for Team Analysis in Soccer

### Paper Track: Soccer

Over the past 25 years, tracking data has become a cornerstone of soccer analysis. Initially, human annotators manually logged the spatial positions of players. This manual effort has now transitioned to automated computer vision (CV) systems using in-venue cameras, with human intervention only required to correct occasional errors due to occlusion or other visual artifacts. Despite its central role in performance analysis, the utility of tracking data has been limited by the fixed positioning of in-venue cameras.  Recent advancements in CV technology, particularly over the last 5 to 10 years, have enabled tracking from broadcast video (i.e., the video viewers watch at home), which now allows analysis across a broader range of leagues worldwide. However, while these CV systems are impressive, they still require significant fine-tuning, and their outputs need to be filtered before the data is ready for downstream analysis. From a CV perspective, ensuring consistent tracking without multiple identity errors and calibrating the field so it remains stable—without shifting, swaying, or rotating (see Figure 1)—are essential. Even when these conditions are met, there are still considerable differences between data captured from in-venue tracking systems and broadcast tracking systems. Our analysis reveals that, on average, 50% of players are out of view from the main camera angle, and during 5-10% of the match, no players are visible due to close-ups, replays, or alternative camera views.

Given soccer's global popularity, researchers and practitioners often apply advanced computer vision algorithms to soccer videos early in their testing. A 20-second clip shared on platforms like LinkedIn may garner attention and give the impression that capturing tracking data is easy. This is problematic for two reasons: (i) It misrepresents the technology’s capabilities—analyzing a full soccer match requires continuous tracking over 5,400 seconds, not just 20 seconds; (ii) Analysts and data users end up spending most of their time validating data instead of analyzing it, which is an inefficient use of their expertise.  In this paper, we present a metric called "Tracking Completeness," which quantifies the readiness of broadcast tracking system outputs compared to in-venue tracking benchmarks. Our practical experiments reveal significant differences between demo-level and production-level capabilities, particularly highlighting the limitations of open-source tools. These tools often lack the essential "last mile" techniques needed to refine data and generate reliable insights. While promising, they fall short of the professional standards required for real-world sports tracking. This research provides a valuable resource, helping clarify the distinctions between open-source demonstrations and fully realized systems in AI-driven sports analytics.

<img width="600" alt="image" src="https://github.com/user-attachments/assets/5183a9ce-d27b-471b-864e-05fa6c50404e">
